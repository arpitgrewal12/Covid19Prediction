# -*- coding: utf-8 -*-
"""Use-Refit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vgtkKDG15cKNb15eZdEzlYw7gIVV2Jm5
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from imblearn.under_sampling import ClusterCentroids

#Making Pipeline
#from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV

# Model/Dataset Split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split

# accuracy information
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import precision_score
from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import recall_score
from sklearn.metrics import make_scorer

from sklearn.utils import resample

#Function to evaluate models and print our relevant statistics/plots
def EvaluateModel(X_true, y_true, y_pred, X_train, y_train, model):
    #Classification Report
    print("________________________________________")
    print("Classification Report\n")
    report = classification_report(y_true,y_pred, output_dict=True)
    report_df = pd.DataFrame(report).transpose().round(3)
    print(report_df)

    #Train and Test Scores
    print("________________________________________")
    print("Train and Test Scores\n")
    print("Training Accuracy Score: ", model.score(X_train, y_train))
    print("Testing Accuracy Score: ", model.score(X_true, y_true))
    
    #Plot Confusion Matrix
    print("________________________________________")
    print("Confusion Matrix\n")
    #plot_confusion_matrix(model,X_true,y_true,cmap='gnuplot',normalize='true')
    #plt.show()

def custom_recall_scorer(y, y_pred):
    # The '0' label contains the decased. Since we want to maximize the recall only on that,
    # we only return that score
    r_score = recall_score(y, y_pred, average=None, labels=[0,1,2,3])
    #print("Score is", r_score)
    return r_score[0]

recall_scorer = make_scorer(custom_recall_scorer, greater_is_better=True)
#recall_scorer

## Read in the dataset
dataset = pd.read_csv('selected-features.csv', sep=',')
print("The total number of records are:", len(dataset))

y = dataset['outcome'].copy() 
X = dataset.drop('outcome', axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)
y_train.value_counts()

#y = dataset['outcome'].copy() 
#X = dataset.drop('outcome', axis=1)
#cc = ClusterCentroids(sampling_strategy='not minority', random_state=1, n_jobs=-1, )
#X_res, y_res = X_res, y_res = cc.fit_resample(X, y)
#X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.25, random_state=1)

print("Resampling with clusters done!")

AB_model = AdaBoostClassifier(random_state=1)
AB_fit = AB_model.fit(X_train, y_train)
y_pred = AB_fit.predict(X_test)
EvaluateModel(X_test, y_test, y_pred, X_train, y_train, AB_fit)

dec_tree_classifiers = list()
# We do not want to have a decision tree that is "too strong" so 
# keep these numbers low.
for n in [1,3]:
  dec_mod = DecisionTreeClassifier(max_depth=n, random_state=1)
  dec_tree_classifiers.append(dec_mod)
num_estimators = [10, 50, 100, 150, 200]
learn_rate = [0.001, 0.01, 0.1, 0.5,1]
parameters = {'base_estimator':dec_tree_classifiers,
              'n_estimators': num_estimators, 
              'learning_rate':learn_rate}

scoring = {'deceased_recall': recall_scorer, 'Accuracy': make_scorer(accuracy_score)}

adaModel = AdaBoostClassifier(random_state=1)

# Create the model

gridCV = GridSearchCV(estimator=adaModel,
                      param_grid=parameters, 
                      verbose=3, 
                      scoring=scoring,
                      refit='deceased_recall', 
                      n_jobs=-1,
                      cv=3)

cv_results = gridCV.fit(X_train, y_train)
print("Finished Grid Search!")

print(cv_results.best_estimator_)